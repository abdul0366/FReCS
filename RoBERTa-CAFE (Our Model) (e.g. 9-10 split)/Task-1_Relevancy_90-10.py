# -*- coding: utf-8 -*-
"""fr_r_90.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GgFJh_2uWbg_3rhEYvhb0IjsOk7hqTNu
"""

# -*- coding: utf-8 -*-
"""fr2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QfesVw00W6vZsp9qCn7_pY6d1-sqn9SW
"""

import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import math

class CrossAttention(nn.Module):
    def __init__(self, input_dim):
        super(CrossAttention, self).__init__()
        self.query = nn.Linear(input_dim, input_dim)
        self.key = nn.Linear(input_dim, input_dim)
        self.value = nn.Linear(input_dim, input_dim)

    def forward(self, x, context):
        q = self.query(x)
        k = self.key(context)
        v = self.value(context)
        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))
        attention = torch.softmax(attention_scores, dim=-1)
        attended = torch.matmul(attention, v)
        return attended + x

class DisentangledAttention(nn.Module):
    def __init__(self, input_dim):
        super(DisentangledAttention, self).__init__()
        self.aspect1_attention = nn.Linear(input_dim, input_dim)
        self.aspect2_attention = nn.Linear(input_dim, input_dim)

    def forward(self, x):
        aspect1 = torch.sigmoid(self.aspect1_attention(x))
        aspect2 = torch.sigmoid(self.aspect2_attention(x))
        return aspect1 * x + aspect2 * x

class MultiHeadAttention(nn.Module):
    def __init__(self, input_dim, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert input_dim % num_heads == 0, "input_dim must be divisible by num_heads"

        self.num_heads = num_heads
        self.depth = input_dim // num_heads
        self.W_q = nn.Linear(input_dim, input_dim)
        self.W_k = nn.Linear(input_dim, input_dim)
        self.W_v = nn.Linear(input_dim, input_dim)
        self.dense = nn.Linear(input_dim, input_dim)

    def split_heads(self, x, batch_size):
        x = x.view(batch_size, -1, self.num_heads, self.depth)
        return x.permute(0, 2, 1, 3)

    def forward(self, x):
        batch_size = x.size(0)
        q = self.split_heads(self.W_q(x), batch_size)
        k = self.split_heads(self.W_k(x), batch_size)
        v = self.split_heads(self.W_v(x), batch_size)

        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.depth)
        attention_weights = torch.softmax(attention_scores, dim=-1)

        output = torch.matmul(attention_weights, v)
        output = output.permute(0, 2, 1, 3).contiguous()
        output = output.view(batch_size, -1, self.num_heads * self.depth)
        return self.dense(output)

class AdaptiveFeedForward(nn.Module):
    def __init__(self, input_dim, ff_dim=2048, depth=2, dropout_rate=0.2):
        super(AdaptiveFeedForward, self).__init__()
        self.layers = nn.ModuleList()

        for i in range(depth):
            in_dim = input_dim if i == 0 else ff_dim // 2  # Account for GLU halving dimension
            self.layers.append(nn.Linear(in_dim, ff_dim))
            self.layers.append(nn.GLU())
            self.layers.append(nn.Dropout(dropout_rate))

        # The last layer needs to project back to the input dimension
        self.layers.append(nn.Linear(ff_dim // 2, input_dim))

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

class DisasterTypeClassifier(nn.Module):
    def __init__(self, num_classes, dropout_rate=0.2):
        super(DisasterTypeClassifier, self).__init__()
        self.roberta = AutoModel.from_pretrained('roberta-base', output_hidden_states=True)
        self.cross_attention = CrossAttention(768)
        self.disentangled_attention = DisentangledAttention(768)
        self.multi_head_attention = MultiHeadAttention(768, 8)
        self.norm1 = nn.LayerNorm(768)
        self.adaptive_ffn = AdaptiveFeedForward(768, 2048, 2, dropout_rate)
        self.norm2 = nn.LayerNorm(768)
        self.dropout = nn.Dropout(dropout_rate)
        self.fc = nn.Linear(768, 1)

    def forward(self, input_ids, attention_mask, additional_context=None):
        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        if additional_context is not None:
            context_output = self.cross_attention(roberta_output, additional_context)
            roberta_output = roberta_output + context_output
        disentangled_output = self.disentangled_attention(roberta_output)
        multi_head_output = self.multi_head_attention(disentangled_output)
        normed_output = self.norm1(multi_head_output + disentangled_output)
        ffn_output = self.adaptive_ffn(normed_output)
        normed_ffn_output = self.norm2(ffn_output + normed_output)
        # pooled_output = self.dropout(normed_ffn_output[:, 0])
        # return self.fc(pooled_output)
        pooled_output = self.dropout(normed_ffn_output[:, 0])
        # For binary classification, we output a single logit
        return self.fc(pooled_output).squeeze(-1)



# Further code for data preprocessing, model training, and evaluation would follow here, similar to the previous snippets.


def preprocess_text(text):
    return text.replace("URL", "").replace("@USER", "").strip()

class TweetDataset(Dataset):
    def __init__(self, tweets, labels, tokenizer, max_len):
        self.tweets = tweets
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.tweets)

    def __getitem__(self, idx):
        tweet = self.tweets[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            tweet,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }
tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)
#tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)
df_train = pd.read_csv("/home/aaadfg/fr/90_10/train_90.csv")
df_test = pd.read_csv("/home/aaadfg/fr/90_10/test_90.csv")

df_train['processed_tweet'] = df_train['Tweet_Text'].apply(preprocess_text)
df_test['processed_tweet'] = df_test['Tweet_Text'].apply(preprocess_text)

# Update this dictionary based on your dataset
encoded_dict = {"Not_Related": 0, "Related": 1}

df_train['event'] = df_train['Informativeness'].map(encoded_dict)
df_test['event'] = df_test['Informativeness'].map(encoded_dict)

train_dataset = TweetDataset(df_train['processed_tweet'].to_numpy(), df_train['event'].to_numpy(), tokenizer, max_len=128)
test_dataset = TweetDataset(df_test['processed_tweet'].to_numpy(), df_test['event'].to_numpy(), tokenizer, max_len=128)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = DisasterTypeClassifier(num_classes=len(encoded_dict), dropout_rate=0.2).to(device)

# # Utilizing multiple GPUs if available
if torch.cuda.device_count() > 1:
    print(f"Let's use {torch.cuda.device_count()} GPUs!")
    model = nn.DataParallel(model)
model.to(device)

# optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
# Use RAdam optimizer
optimizer = torch.optim.RAdam(model.parameters(), lr=2e-5)

loss_fn = nn.BCEWithLogitsLoss()

# Learning rate scheduler
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)


# Training loop with scheduler step
for epoch in range(10):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        # Convert labels to floating-point type
        labels = batch['labels'].to(device).float()

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    scheduler.step()
    print(f"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}")

# Evaluation
model.eval()
predictions, true_labels = [], []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask)

        # Apply sigmoid to convert outputs to probabilities
        probs = torch.sigmoid(outputs).squeeze()

        # Convert probabilities to binary predictions using a threshold (e.g., 0.5)
        predicted = (probs > 0.5).long()

        predictions.extend(predicted.cpu().tolist())
        true_labels.extend(labels.cpu().numpy().tolist())  # Adjusted here

print(classification_report(true_labels, predictions, target_names=['Not Related', 'Related']))


# Confusion Matrix Visualization
plt.figure(figsize=(10, 10))
cm = confusion_matrix(true_labels, predictions)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.ylabel('Actual Labels')
plt.xlabel('Predicted Labels')
plt.savefig('/home/aaadfg/fr/fr_r.png')
plt.show()


# Saving the model to ONNX format
model.eval()  # Ensure the model is in evaluation mode
dummy_input_ids = torch.randint(0, 2000, (64, 128))  # Adjust as necessary
dummy_attention_mask = torch.ones(64, 128)  # Adjust as necessary
onnx_file_path = "fr_r_classifier.onnx"

torch.onnx.export(model,
                  (dummy_input_ids, dummy_attention_mask),
                  onnx_file_path,
                  export_params=True,
                  opset_version=11,
                  do_constant_folding=True,
                  input_names=['input_ids', 'attention_mask'],
                  output_names=['output'],
                  dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},
                                'attention_mask': {0: 'batch_size', 1: 'sequence'},
                                'output': {0: 'batch_size'}})

print(f"Model has been saved to {onnx_file_path}.")